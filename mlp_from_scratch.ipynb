{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP From Scratch\n",
    "Homework 1  \n",
    "ECSE 552  \n",
    "Gian Favero and Mohamed Mohamed  \n",
    "February 5th, 2024  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object of this notebook is to construct an MLP network from scratch without using autodifferentiation or optimization capabilities of higher-level libraries (PyTorch, TensorFlow, etc.). In other words, the gradients must be calculated and propogated through the network so as to implement stochastic gradient descent manually.\n",
    "\n",
    "For simplification, the network has some constraints:\n",
    "\n",
    "- The neural network has two hidden layers\n",
    "- The network may have a fixed architecture\n",
    "- Network parameters can be randomly initialized\n",
    "- The output has 3 units\n",
    "- All layers have sigmoid activation functions\n",
    "- Batch size is 1\n",
    "- The loss function is sum of squared errors (SSE) across the three outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "We begin by importing the necessary libraries for this implementation. In our case all we need is NumPy for the algorithmic operations and matplotlib for plotting loss and validation curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
